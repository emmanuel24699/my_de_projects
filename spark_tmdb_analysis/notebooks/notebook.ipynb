{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430cbc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, explode, collect_list, concat_ws, when, lit, to_date, size, first, isnan\n",
    "from pyspark.sql.types import FloatType\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import json\n",
    "import sys\n",
    "sys.path.append(str(Path('../scripts')))\n",
    "from config import MOVIE_IDS,TMDB_API_KEY,BASE_URL,RAW_DATA_DIR, PROCESSED_DATA_DIR\n",
    "from api_fetch import initialize_spark, create_movie_dataframe\n",
    "from data_cleaning import clean_data\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce13e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-20 21:18:09,616 - INFO - Spark session initialized with custom configurations\n",
      "2025-04-20 21:18:09,697 - INFO - Loaded cached data from data\\raw\\raw_movies_20250420_210921.json\n",
      "2025-04-20 21:18:13,489 - WARNING - Attempt 1 failed for movie ID 0: 404 Client Error: Not Found for url: https://api.themoviedb.org/3/movie/0?api_key=98eb78059acf277a9397adb9a104b869&append_to_response=credits\n",
      "2025-04-20 21:18:15,586 - WARNING - Attempt 2 failed for movie ID 0: 404 Client Error: Not Found for url: https://api.themoviedb.org/3/movie/0?api_key=98eb78059acf277a9397adb9a104b869&append_to_response=credits\n",
      "2025-04-20 21:18:18,777 - WARNING - Attempt 3 failed for movie ID 0: 404 Client Error: Not Found for url: https://api.themoviedb.org/3/movie/0?api_key=98eb78059acf277a9397adb9a104b869&append_to_response=credits\n",
      "2025-04-20 21:18:18,778 - ERROR - Max retries reached for movie ID 0\n",
      "2025-04-20 21:18:18,781 - WARNING - Skipping movie ID 0 due to fetch failure\n",
      "2025-04-20 21:18:18,782 - INFO - Using cached data for movie ID 299534\n",
      "2025-04-20 21:18:18,784 - INFO - Using cached data for movie ID 19995\n",
      "2025-04-20 21:18:18,785 - INFO - Using cached data for movie ID 140607\n",
      "2025-04-20 21:18:18,786 - INFO - Using cached data for movie ID 299536\n",
      "2025-04-20 21:18:18,787 - INFO - Using cached data for movie ID 597\n",
      "2025-04-20 21:18:18,788 - INFO - Using cached data for movie ID 135397\n",
      "2025-04-20 21:18:18,789 - INFO - Using cached data for movie ID 420818\n",
      "2025-04-20 21:18:18,790 - INFO - Using cached data for movie ID 24428\n",
      "2025-04-20 21:18:18,791 - INFO - Using cached data for movie ID 168259\n",
      "2025-04-20 21:18:18,792 - INFO - Using cached data for movie ID 99861\n",
      "2025-04-20 21:18:18,793 - INFO - Using cached data for movie ID 284054\n",
      "2025-04-20 21:18:18,794 - INFO - Using cached data for movie ID 12445\n",
      "2025-04-20 21:18:18,795 - INFO - Using cached data for movie ID 181808\n",
      "2025-04-20 21:18:18,796 - INFO - Using cached data for movie ID 330457\n",
      "2025-04-20 21:18:18,797 - INFO - Using cached data for movie ID 351286\n",
      "2025-04-20 21:18:18,800 - INFO - Using cached data for movie ID 109445\n",
      "2025-04-20 21:18:18,801 - INFO - Using cached data for movie ID 321612\n",
      "2025-04-20 21:18:18,802 - INFO - Using cached data for movie ID 260513\n",
      "2025-04-20 21:18:19,290 - INFO - Saved combined movie data to data\\raw\\raw_movies_20250420_211818.json\n",
      "2025-04-20 21:18:19,292 - INFO - Latest timestamp recorded: 20250420_211818\n",
      "2025-04-20 21:18:34,404 - INFO - Created Spark DataFrame with 18 rows\n",
      "2025-04-20 21:18:34,405 - INFO - Failed IDs: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 7587)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\skele\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"C:\\Users\\skele\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"C:\\Users\\skele\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"C:\\Users\\skele\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"d:\\Downloads\\www.amalitech.org\\Projects\\my_de_projects\\spark_tmdb_analysis\\spark_venv\\Lib\\site-packages\\pyspark\\accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"d:\\Downloads\\www.amalitech.org\\Projects\\my_de_projects\\spark_tmdb_analysis\\spark_venv\\Lib\\site-packages\\pyspark\\accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"d:\\Downloads\\www.amalitech.org\\Projects\\my_de_projects\\spark_tmdb_analysis\\spark_venv\\Lib\\site-packages\\pyspark\\accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Downloads\\www.amalitech.org\\Projects\\my_de_projects\\spark_tmdb_analysis\\spark_venv\\Lib\\site-packages\\pyspark\\serializers.py\", line 594, in read_int\n",
      "    length = stream.read(4)\n",
      "             ^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\skele\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "spark = initialize_spark()\n",
    "raw_df = create_movie_dataframe(MOVIE_IDS, TMDB_API_KEY, BASE_URL, RAW_DATA_DIR, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e6737e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+---------------------+---------+--------------------+--------------------+------+---------+--------------+-----------------+-----------------+--------------------+----------+--------------------+--------------------+--------------------+------------+----------+-------+--------------------+--------+------------------+-----------------+-----+------------+----------+--------------------+\n",
      "|adult|       backdrop_path|belongs_to_collection|   budget|              genres|            homepage|    id|  imdb_id|origin_country|original_language|   original_title|            overview|popularity|         poster_path|production_companies|production_countries|release_date|   revenue|runtime|    spoken_languages|  status|           tagline|            title|video|vote_average|vote_count|             credits|\n",
      "+-----+--------------------+---------------------+---------+--------------------+--------------------+------+---------+--------------+-----------------+-----------------+--------------------+----------+--------------------+--------------------+--------------------+------------+----------+-------+--------------------+--------+------------------+-----------------+-----+------------+----------+--------------------+\n",
      "|false|/7RyHsO4yDXtBv1zU...| {86311, The Aveng...|356000000|[{12, Adventure},...|https://www.marve...|299534|tt4154796|          [US]|               en|Avengers: Endgame|After the devasta...|   31.7543|/ulzhLuWrPK07P1Yk...|[{420, /hUzeosd33...|[{US, United Stat...|  2019-04-24|2799439100|    181|[{English, en, En...|Released|Avenge the fallen.|Avengers: Endgame|false|       8.237|     26243|{[{false, 2, 3223...|\n",
      "+-----+--------------------+---------------------+---------+--------------------+--------------------+------+---------+--------------+-----------------+-----------------+--------------------+----------+--------------------+--------------------+--------------------+------------+----------+-------+--------------------+--------+------------------+-----------------+-----+------------+----------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19983e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = (raw_df\n",
    "    # Drop columns\n",
    "    .drop(*columns_to_drop)\n",
    "    \n",
    "    # Process JSON columns\n",
    "    .withColumn(\"belongs_to_collection\", \n",
    "        F.when(F.col(\"belongs_to_collection\").isNotNull(), \n",
    "              F.col(\"belongs_to_collection.name\")))\n",
    "    \n",
    "    .withColumn(\"genres\", \n",
    "        F.array_join(F.transform(F.col(\"genres\"), \n",
    "                               lambda x: x[\"name\"]), \" | \"))\n",
    "    \n",
    "    .withColumn(\"spoken_languages\", \n",
    "        F.array_join(F.transform(F.col(\"spoken_languages\"), \n",
    "                               lambda x: x[\"iso_639_1\"]), \" | \"))\n",
    "    \n",
    "    .withColumn(\"production_countries\", \n",
    "        F.array_join(F.transform(F.col(\"production_countries\"), \n",
    "                               lambda x: x[\"iso_3166_1\"]), \" | \"))\n",
    "    \n",
    "    .withColumn(\"production_companies\", \n",
    "        F.array_join(F.transform(F.col(\"production_companies\"), \n",
    "                               lambda x: x[\"name\"]), \" | \"))\n",
    "    \n",
    "    .withColumn(\"origin_country\", \n",
    "        F.array_join(F.array_sort(F.col(\"origin_country\")), \" | \"))\n",
    "    \n",
    "    # Process credits\n",
    "    .withColumn(\"cast\", \n",
    "        F.array_join(F.transform(F.col(\"credits.cast\"), \n",
    "                               lambda x: x[\"name\"]), \", \"))\n",
    "    \n",
    "    .withColumn(\"cast_size\", \n",
    "        F.size(F.col(\"credits.cast\")))\n",
    "    \n",
    "    .withColumn(\"director\", \n",
    "        F.array_join(\n",
    "            F.filter(F.col(\"credits.crew\"), \n",
    "                    lambda x: x[\"job\"] == \"Director\")[\"name\"], \n",
    "            \", \"))\n",
    "    \n",
    "    .withColumn(\"crew_size\", \n",
    "        F.size(F.col(\"credits.crew\")))\n",
    "    \n",
    "    # Drop original credits\n",
    "    .drop(\"credits\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa67f230",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
